{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install numpy==1.20.3\n!pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install opencv-python\n!pip install matplotlib==3.4.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex\n!cd apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n!rm -rf ./apex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export CUDA_HOME=/usr/local/cuda-11.0 \n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install -c conda-forge nvidia-apex -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n!pip install torchtext==0.9.1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip freeze","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 freeze > requirements.txt  # Python3\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install timm==0.3.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from apex import amp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.remove('data.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.unpack_archive('data.zip', \"./\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nshutil.rmtree(\"./data/train_segmentation\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip uninstall opencv_python_headless -y\n!pip install opencv-contrib-python-headless","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\ntps = cv2.createThinPlateSplineShapeTransformer()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\nimport cv2\nimport os\nimport json\nfrom matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:49:45.515059Z","iopub.execute_input":"2022-03-10T23:49:45.515715Z","iopub.status.idle":"2022-03-10T23:49:45.588775Z","shell.execute_reply.started":"2022-03-10T23:49:45.515674Z","shell.execute_reply":"2022-03-10T23:49:45.587921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install patool pyunpack","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt-get install unrar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1LGm-ZWwesqRJwuzXW0ZgPA4JSF-HGdoD","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip synth_img.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.mkdir(\"./data\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.rmtree('./data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyunpack import Archive\nArchive('./train.rar').extractall('./data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./data/train/labels.json') as f:\n    train_data = json.load(f)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('data/train/labels.json') as f:\n    train_data = json.load(f)\n\ntrain_data = [(k, v) for k, v in train_data.items()]\nprint('train len', len(train_data))\n\nsplit_coef = 0.75\ntrain_len = int(len(train_data)*split_coef)\n\ntrain_data_splitted = train_data[:train_len]\nval_data_splitted = train_data[train_len:]\n\nprint('train len after split', len(train_data_splitted))\nprint('val len after split', len(val_data_splitted))\n\n\nwith open('data/train/train_labels_splitted.json', 'w') as f:\n    json.dump(dict(train_data_splitted), f)\n    \nwith open('data/train/val_labels_splitted.json', 'w') as f:\n    json.dump(dict(val_data_splitted), f)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:49:52.164161Z","iopub.execute_input":"2022-03-10T23:49:52.164463Z","iopub.status.idle":"2022-03-10T23:49:52.372761Z","shell.execute_reply.started":"2022-03-10T23:49:52.164429Z","shell.execute_reply":"2022-03-10T23:49:52.371972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('./data/train')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nlabels = pd.read_csv(\"./data/train/labels.csv\")\ndata  = dict(zip(labels.file_name, labels.text))\n\ntrain_data = [(k, v) for k, v in data.items()]\nprint('train len', len(train_data))\n\nsplit_coef = 0.60\ntrain_len = int(len(train_data)*split_coef)\n\ntrain_data_splitted = train_data[:train_len]\nval_data_splitted = train_data[train_len:]\n\nprint('train len after split', len(train_data_splitted))\nprint('val len after split', len(val_data_splitted))\n\n\nwith open('data/train_labels_splitted.json', 'w') as f:\n    json.dump(dict(train_data_splitted), f)\n    \nwith open('data/val_labels_splitted.json', 'w') as f:\n    json.dump(dict(val_data_splitted), f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir(\"./with_aug\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# функция которая помогает объединять картинки и таргет-текст в батч\ndef collate_fn(batch):\n    images, texts, enc_texts = zip(*batch)\n    images = torch.stack(images, 0)\n    text_lens = torch.LongTensor([len(text) for text in texts])\n    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n    return images, texts, enc_pad_texts, text_lens\n\n\ndef get_data_loader(\n    transforms, json_path, root_path, tokenizer, batch_size, drop_last\n):\n    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n    data_loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        pin_memory = True,\n        collate_fn=collate_fn,\n        batch_size=batch_size,\n        num_workers=8,\n    )\n    return data_loader\n\n\nclass OCRDataset(Dataset):\n    def __init__(self, json_path, root_path, tokenizer, transform=None):\n        super().__init__()\n        self.transform = transform\n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        self.data_len = len(data)\n\n        self.img_paths = []\n        self.texts = []\n        for img_name, text in data.items():\n            self.img_paths.append(os.path.join(root_path, img_name))\n            self.texts.append(text)\n        self.enc_texts = tokenizer.encode(self.texts)\n\n    def __len__(self):\n        return self.data_len\n\n    def __getitem__(self, idx):\n        img_path = self.img_paths[idx]\n        text = self.texts[idx]\n        enc_text = torch.LongTensor(self.enc_texts[idx])\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, text, enc_text\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:49:55.284926Z","iopub.execute_input":"2022-03-10T23:49:55.285499Z","iopub.status.idle":"2022-03-10T23:49:55.300818Z","shell.execute_reply.started":"2022-03-10T23:49:55.285458Z","shell.execute_reply":"2022-03-10T23:49:55.30007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nconfig_json = {\n    \"alphabet\": '@!\"\\'()+,-./0123456789:;=?I[]ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№IZVX ',\n    \"save_dir\": \"./with_aug\",\n    \"num_epochs\": 50,\n    \"learning_rate\": 0.00001,\n    \"FPN_ON\": True,\n    \"ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG\": True,\n    \"BBOX_REG_LOSS_TYPE\": \"giou\",\n    \"PIXEL_STD\" : [57.375, 57.120, 58.395], \n    \"steps\": 6000,\n    \"feature_extract\": True,\n    \"pin_memory\": True,\n    \"NESTEROV\": True,\n    \"image\": {\n        \"width\": 256,\n        \"height\": 32\n    },\n    \"train\": {\n        \"root_path\": \"./data/train/images\",\n        \"json_path\": \"./data/train/train_labels_splitted.json\",\n        \"batch_size\": 64\n    },\n    \"val\": {\n        \"root_path\": \"./data/train/images\",\n        \"json_path\": \"./data/train/val_labels_splitted.json\",\n        \"batch_size\": 64\n    }\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:49:59.409957Z","iopub.execute_input":"2022-03-10T23:49:59.410573Z","iopub.status.idle":"2022-03-10T23:49:59.441841Z","shell.execute_reply.started":"2022-03-10T23:49:59.410528Z","shell.execute_reply":"2022-03-10T23:49:59.441024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install straug \n!pip install opencv-contrib-python-headless\n!pip install magickwand -y\n!apt-get install libmagickwand-dev -y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OOV_TOKEN = '<OOV>'\nCTC_BLANK = '<BLANK>'\n\n\ndef get_char_map(alphabet):\n    \"\"\"Make from string alphabet character2int dict.\n    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n    char_map[CTC_BLANK] = 0\n    char_map[OOV_TOKEN] = 1\n    return char_map\n\n\nclass Tokenizer:\n    \"\"\"Class for encoding and decoding string word to sequence of int\n    (and vice versa) using alphabet.\"\"\"\n\n    def __init__(self, alphabet):\n        self.char_map = get_char_map(alphabet)\n        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n\n    def encode(self, word_list):\n        \"\"\"Returns a list of encoded words (int).\"\"\"\n        enc_words = []\n        for word in word_list:\n            enc_words.append(\n                [self.char_map[char] if char in self.char_map\n                 else self.char_map[OOV_TOKEN]\n                 for char in word]\n            )\n            \n        return enc_words\n\n    def get_num_chars(self):\n        return len(self.char_map)\n\n    def decode(self, enc_word_list):\n        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n        repeating characters. Also skip out of vocabulary token.\"\"\"\n        dec_words = []\n        for word in enc_word_list:\n            word_chars = ''\n            for idx, char_enc in enumerate(word):\n                # skip if blank symbol, oov token or repeated characters\n                if (\n                    char_enc != self.char_map[OOV_TOKEN]\n                    and char_enc != self.char_map[CTC_BLANK]\n                    # idx > 0 to avoid selecting [-1] item\n                    and not (idx > 0 and char_enc == word[idx - 1])\n                ):\n                    word_chars += self.rev_char_map[char_enc]\n            dec_words.append(word_chars)\n        return dec_words","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:01.987574Z","iopub.execute_input":"2022-03-10T23:50:01.988326Z","iopub.status.idle":"2022-03-10T23:50:02.001231Z","shell.execute_reply.started":"2022-03-10T23:50:01.988285Z","shell.execute_reply":"2022-03-10T23:50:02.000468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install ASR-metrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/NVIDIA/apex\n%cd apex\n!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ./worfking","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from apex import amp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ASR_metrics import utils as metrics\n\ndef get_accuracy(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        scores.append(metrics.calculate_cer(true, pred))\n    avg_score = np.mean(scores)\n    return avg_score","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:11.6383Z","iopub.execute_input":"2022-03-10T23:50:11.638598Z","iopub.status.idle":"2022-03-10T23:50:11.646764Z","shell.execute_reply.started":"2022-03-10T23:50:11.638566Z","shell.execute_reply":"2022-03-10T23:50:11.645882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wand\nfrom straug.warp import *\nfrom straug.geometry import *\nfrom straug.blur import *\n\nfrom PIL import Image\nimport copy\nimport cv2\nimport numpy as np\n\nimg = Image.open(\"./data/train/images/0.png\").convert(\"RGB\")\n\nimg = MotionBlur()(img, mag=2)\nopen_cv_image = np.array(img) \nopen_cv_image = open_cv_image[:, :, ::-1].copy() \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from straug.warp import *\nfrom straug.geometry import *\nfrom straug.blur import *\nfrom straug.camera import *\nfrom straug.noise import *\nfrom straug.process import *\nimport numpy as np\nfrom PIL import Image\nimport random\n\n\nclass ChadAug:\n    def __init__(self, n=1, aug_pol=None):\n        self.aug_pol = [\n                [ShotNoise()], [Perspective()],                    \n                [DefocusBlur(), MotionBlur()],                     # Blur\n                [Brightness(), JpegCompression(), Pixelate()],     # Camera\n                [Sharpness(), Color(), AutoContrast()] \n        ]\n\n        if aug_pol is not None:\n            self.aug_pol = aug_pol\n        \n        self.n = n\n\n    def roll_augs(self):\n        aug_types = random.sample(self.aug_pol, self.n)\n        return [random.choice(a) for a in aug_types]\n\n    def __call__(self, img):\n        img = Image.fromarray(img[:, :, ::-1])\n      \n        augs = self.roll_augs()\n        #print(augs)\n        for aug in augs:\n            img = aug(img)\n      \n        img = np.array(img)[:, :, ::-1]\n        return img","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:17.673036Z","iopub.execute_input":"2022-03-10T23:50:17.673614Z","iopub.status.idle":"2022-03-10T23:50:18.96887Z","shell.execute_reply.started":"2022-03-10T23:50:17.673575Z","shell.execute_reply":"2022-03-10T23:50:18.967877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MixedAug:\n  def __init__(self, n=1, aug_pol=None):\n\n    self.aug_pol = [\n                                           # Warp\n                [DefocusBlur(), MotionBlur()],                     # Blur\n                [Brightness(), JpegCompression(), Pixelate()],     # Camera\n                [Sharpness(), Color(), AutoContrast()]             # Process\n        ] \n\n    if aug_pol is not None:\n        self.aug_pol = aug_pol\n    \n    self.n = n\n\n  def roll_mixed_augs(self):\n\n      aug_types = random.sample(self.aug_pol, self.n)\n      return [random.choice(a) for a in aug_types]\n\n  def __call__(self, img):\n        \n        w, h = img.size\n        img = np.asarray(img)\n\n        half = w//2\n        left_part = img[:, :half] \n        right_part = img[:, half:]\n\n        augs = self.roll_mixed_augs()\n        #print(augs)\n        for aug in augs:\n\n            img_l = aug(Image.fromarray(left_part[:, :, ::-1]))\n            img_r = aug(Image.fromarray(right_part[:, :, ::-1]))\n\n            list_im = [img_l, img_r]\n            imgs = [i for i in list_im ]\n\n            min_shape = sorted( [(np.sum(i.size), i.size ) for i in imgs])[0][1]\n            imgs_comb = np.hstack((np.asarray( i.resize(min_shape) ) for i in imgs))\n\n            img = np.array(imgs_comb)[:, :, ::-1]\n        return img","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import  AutoAugmentPolicy\nfrom matplotlib import pyplot as plt\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import AutoAugment  \nimport albumentations as A\nimport torch.optim as optim\nimport wand\nfrom straug.warp import *\nfrom straug.geometry import *\nfrom straug.blur import *\nfrom torchvision.transforms import RandomAffine\nfrom torchvision.transforms import RandomCrop\nfrom torchvision.transforms import RandomRotation\nfrom torchvision import transforms, models\nfrom tqdm import tqdm\nimport copy\nimport random \nimport PIL\nfrom PIL import Image\n\n\nclass ImageFloppaResize:\n    def __init__(self, height, width, pad_color=(0, 0, 0), border_m=cv2.BORDER_CONSTANT):\n        # Resize to values\n        self.height = height\n        self.width = width\n        self.pad_color = pad_color\n        self.border_m = border_m\n\n    def __call__(self, image):\n        img_h, img_w = image.shape[:-1:]\n        resize_f = min(self.height / img_h, self.width / img_w)\n        dim = (int(img_w * resize_f), int(img_h * resize_f))\n        image = cv2.resize(image, dim, interpolation=cv2.INTER_LINEAR)\n\n        img_h, img_w = image.shape[:-1:]\n        pad_b = self.height - img_h if self.height - img_h > 0 else 0\n        pad_r = self.width - img_w if self.width - img_w > 0 else 0\n        image = cv2.copyMakeBorder(image.copy(), 0, pad_b, 0, pad_r, self.border_m, value=self.pad_color)\n        return image\n    \nclass MagickAug:\n\n  def __call__(self, img):\n\n    img = Image.fromarray(img)\n    img = Invert()(img, mag=2)\n    img = np.array(img) \n    img = img[:, :, ::-1].copy() \n    return img\n\n\n\n\nclass Normalize:\n    def __call__(self, img):\n        img = img.astype(np.float32) / 255\n        return img\n\nclass Alumentated:\n  def __call__(self, img):\n\n    transform = A.Compose([\n            A.CLAHE(clip_limit=1.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n            A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.1),\n            A.JpegCompression(quality_lower=35, p=0.5),\n        ], p=1.0)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    transformed = transform(image=img)\n    img = transformed[\"image\"]\n    return img \n\nclass ToTensor:\n    def __call__(self, arr):\n        arr = torch.from_numpy(arr)\n        return arr\n\n\nclass MoveChannels:\n\n    def __init__(self, to_channels_first=True):\n        self.to_channels_first = to_channels_first\n\n    def __call__(self, image):\n        if self.to_channels_first:\n            return np.moveaxis(image, -1, 0)\n        else:\n            return np.moveaxis(image, 0, -1)\n\n\nclass ImageResize:\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, image):\n        image = cv2.resize(image, (self.width, self.height),\n                           interpolation=cv2.INTER_LINEAR)\n        return image\n\n\n\ndef get_train_transforms(height, width):\n    transforms = torchvision.transforms.Compose([\n        ChadAug(),\n        ImageFloppaResize(height, width),\n        MoveChannels(to_channels_first=True),\n        Normalize(),\n        ToTensor(),\n    ])\n    return transforms\n\n\ndef get_val_transforms(height, width):\n    transforms = torchvision.transforms.Compose([\n        ImageFloppaResize(height, width),\n        MoveChannels(to_channels_first=True),\n        Normalize(),\n        ToTensor(),\n       \n  \n    ])\n    return transforms","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:23.42388Z","iopub.execute_input":"2022-03-10T23:50:23.424172Z","iopub.status.idle":"2022-03-10T23:50:23.57949Z","shell.execute_reply.started":"2022-03-10T23:50:23.424138Z","shell.execute_reply":"2022-03-10T23:50:23.578776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade efficientnet-pytorch \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"q =  torch.tensor([[3, 64, 32, 256]])\nprint(q.permute(1, 0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install onnx","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install timm==0.5.4 \nfrom timm import create_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import timm\nimport timm.optim\nfrom timm import create_model","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:39.576091Z","iopub.execute_input":"2022-03-10T23:50:39.576578Z","iopub.status.idle":"2022-03-10T23:50:40.136235Z","shell.execute_reply.started":"2022-03-10T23:50:39.576538Z","shell.execute_reply":"2022-03-10T23:50:40.135432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install catboost==0.10.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nmodel_с = CatBoostRegressor(iterations=700, l2_leaf_reg=0.07, task_type='GPU',\n                          learning_rate=0.0001, silent=True, has_time=True, boosting_type= 'Plain',\n                          depth=5, loss_function='MultiRMSE',)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T00:14:09.747504Z","iopub.execute_input":"2022-03-11T00:14:09.747816Z","iopub.status.idle":"2022-03-11T00:14:09.754171Z","shell.execute_reply.started":"2022-03-11T00:14:09.74778Z","shell.execute_reply":"2022-03-11T00:14:09.752826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = nn.Linear(20, 30)\ninput_m = torch.randn(128, 20)\noutput = m(input_m)\noutput","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models as models\nimport onnx\nmodel_name = \"convnext_base_in22k\"\nfrom catboost import CatBoostClassifier, Pool, sum_models\n\ndef get_resnet50_backbone(pretrained=True):\n    m = timm.create_model(model_name, pretrained=True)\n    input_conv = nn.Conv2d(3, 128, 7, 3, bias=False)\n    blocks = [input_conv, \n               m.stages]\n    \n    return nn.Sequential(*blocks)\n\n\n\nclass BiLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size, hidden_size, num_layers,\n            dropout=dropout, batch_first=True, bidirectional=True)\n    \n    def forward(self, x):\n        out, _ = self.lstm(x)\n        return out\n\n\n\ndef make_divisible(v, divisor=8, min_value=None, round_limit=.9):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < round_limit * v:\n        new_v += divisor\n    return new_v\n\n\nclass DenseNetAtt(nn.Module):\n    def __init__(\n        self, number_class_symbols, time_feature_count=32, lstm_hidden=256,\n        lstm_len=3,\n    ):\n        super().__init__()\n        self.feature_extractor = get_resnet50_backbone(pretrained=True)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(\n            (time_feature_count, time_feature_count))\n        \n        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(lstm_hidden *2, time_feature_count,),\n            nn.GELU(),\n            nn.Dropout(0.2),\n            nn.Linear(time_feature_count, number_class_symbols)\n        )\n\n \n    def forward(self, x):\n   \n        x = self.feature_extractor(x)\n        b, c, h, w = x.size()\n        x = x.view(b, c * h, w)\n        x = self.avg_pool(x)\n        x = x.transpose(1, 2)\n        x = self.bilstm(x)\n        x = self.classifier(x)\n        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n  \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2022-03-11T00:13:44.400023Z","iopub.execute_input":"2022-03-11T00:13:44.400468Z","iopub.status.idle":"2022-03-11T00:13:44.417013Z","shell.execute_reply.started":"2022-03-11T00:13:44.400429Z","shell.execute_reply":"2022-03-11T00:13:44.416105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(config_json)","metadata":{"execution":{"iopub.status.busy":"2022-03-11T00:14:13.529519Z","iopub.execute_input":"2022-03-11T00:14:13.530389Z","iopub.status.idle":"2022-03-11T00:14:18.050387Z","shell.execute_reply.started":"2022-03-11T00:14:13.530339Z","shell.execute_reply":"2022-03-11T00:14:18.04897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'./model-4-0.3787.ckpt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda install cudatoolkit==11.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.clip_grad import clip_grad_value_\nfrom torch.optim import optimizer\nfrom ASR_metrics import utils as metrics\nfrom torch.nn import CTCLoss\nfrom torch import Tensor\nimport time\nimport tqdm\n\ntorch.manual_seed(375451*70)\n\ndef get_accuracy(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        scores.append(true == pred)\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef get_cer(y_true, y_pred):\n    scores = []\n    for true, pred in zip(y_true, y_pred):\n        scores.append(metrics.calculate_cer(true, pred))\n    avg_score = np.mean(scores)\n    return avg_score\n\n\ndef val_loop(data_loader, model, tokenizer, device):\n    acc_avg = AverageMeter()\n    cer_avg = AverageMeter()\n    model.eval()\n    for images, texts, _, _ in data_loader:\n        \n        batch_size = len(texts)\n        \n        text_preds = predict(images, model, tokenizer, device)\n        acc_avg.update(get_accuracy(texts, text_preds), batch_size)\n        cer_avg.update(get_cer(texts, text_preds), batch_size)\n\n    return acc_avg.avg, cer_avg.avg\n\ndef train_loop(data_loader, model, criterion, optimizer, epoch):\n    loss_avg = AverageMeter()\n    model.train()\n    print(\"train loop\")\n    for images, texts, enc_pad_texts, text_lens in tqdm.tqdm(data_loader):\n        model.zero_grad()\n        \n        images = images.to(DEVICE)\n        batch_size = len(texts)\n    \n        \n        output = model(images)\n        output_lenghts = torch.full(\n            size=(output.size(1),),\n            fill_value=output.size(0),\n            dtype=torch.long\n        )\n        loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n        loss.backward(retain_graph=True)\n        \n        #with amp.scale_loss(loss, optimizer) as scaled_loss:\n            #scaled_loss.backward()\n        loss_avg.update(loss.item(), batch_size)\n        \n    \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n        \n        optimizer.step()\n    \n    for param_group in optimizer.param_groups:\n        lr = param_group['lr']\n    print(f'Epoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n    return loss_avg.avg\n\n\n\n\ndef predict(images, model, tokenizer, device):\n    model.eval()\n    images = images.to(device)\n    with torch.no_grad():\n        output = model(images)\n    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n    text_preds = tokenizer.decode(pred)\n    return text_preds\n\n\ndef get_loaders(tokenizer, config):\n    train_transforms = get_train_transforms(\n        height=config['image']['height'],\n        width=config['image']['width']\n    )\n    train_loader = get_data_loader(\n        json_path=config['train']['json_path'],\n        root_path=config['train']['root_path'],\n        transforms=train_transforms,\n        tokenizer=tokenizer,\n        batch_size=config['train']['batch_size'],\n        drop_last=True\n    )\n    val_transforms = get_val_transforms(\n        height=config['image']['height'],\n        width=config['image']['width']\n    )\n    val_loader = get_data_loader(\n        transforms=val_transforms,\n        json_path=config['val']['json_path'],\n        root_path=config['val']['root_path'],\n        tokenizer=tokenizer,\n        batch_size=config['val']['batch_size'],\n        drop_last=False\n    )\n    return train_loader, val_loader\n\n\ndef train(config):\n    tokenizer = Tokenizer(config['alphabet'])\n    os.makedirs(config['save_dir'], exist_ok=True)\n    train_loader, val_loader = get_loaders(tokenizer, config)\n\n    model = DenseNetAtt(number_class_symbols=tokenizer.get_num_chars())\n   \n    #model.load_state_dict(torch.load(\"./model-6-0.6521.ckpt\"), strict=False)\n    model.to(DEVICE)\n\n    criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.000125)\n  \n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', factor=0.5, patience=2)\n    #model, optimizer = amp.initialize(model, optimizer, opt_level=\"O0\")\n\n    best_acc = -np.inf\n    acc_avg, cer_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n    \n    epoch_time_m = 0\n    #print(val_loop(val_loader, model, tokenizer, DEVICE))\n    for epoch in range(config['num_epochs']):\n        timer = time.time()\n        print(\"\\nEpoch\", epoch, \"Previous took\", epoch_time_m, \"minutes\")\n        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n        acc_avg, cer_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n        print(f'acc: {acc_avg}; cer: {cer_avg};')\n        scheduler.step(acc_avg)\n        \n        epoch_time_m = int((time.time() - timer)/60)\n        if acc_avg > best_acc:\n            best_acc = acc_avg\n            model_save_path = os.path.join(\n                config['save_dir'], f'model-{epoch}-{acc_avg:.4f}.ckpt')\n            torch.save(model.state_dict(), model_save_path)\n            print('Model weights saved')","metadata":{"execution":{"iopub.status.busy":"2022-03-10T23:50:54.12707Z","iopub.execute_input":"2022-03-10T23:50:54.127498Z","iopub.status.idle":"2022-03-10T23:50:54.157057Z","shell.execute_reply.started":"2022-03-10T23:50:54.127461Z","shell.execute_reply":"2022-03-10T23:50:54.156332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export CMAKE_ARGS='-DCMAKE_VERBOSE_MAKEFILE=ON'\n!export VERBOSE=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}